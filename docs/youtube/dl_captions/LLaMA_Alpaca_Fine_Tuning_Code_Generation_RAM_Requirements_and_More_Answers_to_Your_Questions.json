{
  "title": "LLaMA & Alpaca: Fine-Tuning, Code Generation, RAM Requirements and More | Answers to Your Questions",
  "summary": "\n    重點1: 影片中解答了關於如何調整Llama模型的所有問題，包括兩個模型是否可以生成代碼、是否使用GPU以及運行模型所需的RAM大小等等。\n總結: 本影片回答了關於如何調整Llama模型的問題，包括兩個模型是否可以生成代碼、是否使用GPU以及運行模型所需的RAM大小等等。此外，影片還提供了如何使用Alpacalora以及Lowering Adaption的細節，來幫助使用者自行調整模型。",
  "dialogue": "in this video I will answer all of your, questions about how to fine tune the, Llama model can both models generate, code do these models use the GPU and how, much RAM is necessary to run the models, and many more questions before we start, I just want to say thank you for all of, your kind words and incredible feedback, on my last video I really appreciate, that and it motivates me so much to keep, going and that's what I'm doing today, with another video and now let's start I, know that not all of the questions will, be relevant to you so what I would, recommend you to do is to check out the, description box because I divided the, video into different chapters and just, picked the questions that are relevant, to you so this way you get the most out, of the video and now let's start with, the first question and the first, question was about the input and output, token limit of the alpaca model as well, as how the instruction following, training data was generated and how the, model got fine-tuned and how we can, fine-tune the model ourselves I first, want to briefly answer the question, about the input and output token limit, and for this I found this open issue in, the Llama repository and one of the, contributor states that the model was, trained with 2048 tokens so you can use, up to that and if you want to use more, tokens fine-tuning the model will be, required to support longer sequences and, now let's get to the fine tuning of the, Llama model and Stanford really shared, everything with us all of it so in this, repository we find all the data they use, the 52 000 instruction following tasks, which they use for fine-tuning the model, so we can have a look inside and see, here we have an instruction give three, tips for staying healthy there is no, input an input could for example be that, we asked the model to summarize the text, then the text will be the input and we, pass to the model the desired output so, it can learn how to behave like a chat, agent what they also shared is how they, generated the data which I think is very, cool because here we can see all the see, tasks they used and also they shared the, prompt which they used to generate the, instruction following tasks of 52 000 so, they used this prompt and pass it to the, old may I Da Vinci model and in this way, they generated all the 52 000, instruction following tasks which are, then used to fine-tune the Llama model, and ended up with a alpaca model which, is truly impressive and comparatively, very easy to get there and now finally I, want to show you how the fine tuning can, be actually done and for this we can see, they shared with us all the hyper, parameters which they used also they, shared how you can run the fine tuning, exactly how they did it I really like, the documentation it's very easy to, follow and I think it makes it very easy, to reproduce their results which I, really like and as you can see here in, the final command you really just have, to replace three different parameters, and then you're basically able to run, the fine tuning the way they did it and, if you ask me I think the hardest part, for fine-tuning the model yourself will, be the data generation part while, looking at this I think fine-tuning the, model looks to me pretty straightforward, regarding the fine tuning I have one, more repository for you by the way if, you like this video so far I would, really appreciate if you give this video, a like And subscribe to my channel and, this repository is called alpacalora and, the name Laura comes from lowering, adaption which is a parameter efficient, fine-tuning strategy which allows you to, instruct tune the Llama model on, consumer Hardware what does this mean, the authors of the repository were able, to run the training or the fine tuning, within five hours on a single RTX 4090, and this is another time really, impressive I want to give you a few more, information about the lowering adaption, and some perks it brings and why you, might should consider using this, implementation for fine tuning and one, of the reasons why the lowering adaption, is really cool is that the authors here, use the gbt3 175 billion parameter model, and by applying the lowering adaption, instead of fine tuning it with Adam so, basically fine-tuning the whole model, all parameters they could reduce the, number of trainable parameters by 10 000, times and the GPU memory requirements by, three times and still perform on par or, better on multiple models like Robert A, D bird a gpd2 and gpg3 I'm not sure if I, pronounced them correctly and I don't, want to give you a deep dive into the, paper but I want to give you a broad, understanding on how this is possible, and I feel like this illustration helps, definitely so imagine we have our, embedding like an input vector and think, of this as a transformer in our large, language model and usually if we would, do a normal fine tuning very update all, the parameters of the model we would, also update this part of the model but, with the lowering adaption what we do is, we freeze all the weights in here so we, won't update the weights inside our, model architecture they're existing at, all we just inject trainable rank, decomposition mattresses into each layer, of the Transformer architecture so those, are the rank decomposition mattresses, that are trainable and maybe you have, seen something similar it works a little, bit like an auto encoder so we heavily, reduce the dimensionality of our weight, mattresses inside the Transformer or, attention matches and this way we have, to update way fewer parameters during, training because all of the model, parameters are untouched during our fine, tuning we just learned this rank, decomposition mattresses but don't touch, the model architecture overall and in, inference we obviously add a little bit, of complexity to the model but I would, guess and practice you won't even feel a, difference and during training the, authors even observed the 25 speed up, compared to full fine tuning the gbd3, Remodel and finally I want to show you, why I even spent so much time expanding, this all to you so now we can see that, this is our Frozen model and those are, our fine-tuned task specific weight so, if you plan to fine tune the Llama model, for multiple tasks it's way easier to, just swap the lower weights instead of, swapping the whole model because this, part is tremendously bigger than this, part for example I already found a, fine-tuned version of the alpaca 13B, version and here you can see the Laura, weights only contain 26 megabytes where, I would guess the fully fine-tuned, alpaca 13B version would have around 9, gigabytes of data so you can see that, this is way more lightweight but also, for swapping between different tasks or, task specific models and on a final note, I want to point out that in this, repository exists a clean version of the, original alpaca training data which you, can see here and what was fixed they, described it in this pull request they, resolve some empty outputs added a few, Chain of Thought examples fixed a few, empty code examples removed instructions, resolved n a outputs make empty input, consistent and fix a few wrong answers, so in case you're planning to fine-tune, the Llama model with training data that, was generated by all my eyes gbt model, you might also want to check out this, pull request to further improve your, data quality and this brings us to the, next question can both models generate, code and I thought before I talk much I, just showed the answer to you and for, this I found the following instruction, in the alpaca Laura Library which states, write a Python program that prints the, first 10 Fibonacci numbers and as you, can see there are different prompts or, results for the alpaca Laura Stanford, alpaca and the text DaVinci 3 model and, what I then did is just copy The Prompt, paste it in the Dalai web interface and, see what it generates, and as we can see the result is not, really satisfying and this got me, thinking wait a minute something looks, wrong because in the alpaca Lara, repository they state that both models, the apocalora and the Stanford alpaca, model and both are able to generate a, Python program so I was thinking why is, that not possible in the Dalai interface, and what I then found is that the, official model weights of the alpaca, model aren't even released by Stanford, so they intend to release the model, weights in the near future and on a side, note also the one for a fine-tuned, version of the larger Lama models which, is really cool so not just the 7B but, that's just on a side note and what I, was then thinking is which version of, the alpaca model got even quantized and, which version are we actually using and, to further investigate the capabilities, of the Llama model I just decided to go, with the alpaca Laura repository and, check out if I can generate code with, the model version that they released and, since this repository is not quantized, and runs on a GPU I created a colop, notebook where cloned the repository I, had a minor issue with the rjson library, when installing the requirements so I, had to install a older version but maybe, at the time you're trying this you don't, have to do this additional step I also, commented it here and then I proxied the, port so we can access our local running, radio app using this link and finally, you just have to run this cell and then, you can click on this link and access, the gradual interface and then finally I, pasted the instruction again executed, The Prompt and here's my result and I, know that some of you commented that, you're not happy with the results of the, alpaca model and it's not even close to, the chat GPT model and now that I see, that there is a difference between the, output I'm getting in this graduate app, and in the dollar interface that we, might not using the original weights or, something went wrong with the, quantization of the alpaca model and, here's what I found regard adding the, Lamar C plus plus file and the author or, the owner of the repository wrote The, Following thing this was hacked in an, evening I have no idea if it works, correctly please do not make conclusions, about the models based on results from, this implementation for all I know it, can be completely wrong this project is, for educational purposes new features, will probably be added mostly through, Community contributions and in another, thread He also mentioned that the, downside is that he haven't had the, chance to compare the outputs at, different stages of the inference so he, has doubts himself about the correctness, of this implementation and maybe we have, encountered here that the code, generation in a quantized model version, is not as good as in the original fine, tune model version but I definitely, don't want to leave a bad word about, this repository because I think this is, a very promising project because if we, were able to run those big models on a, CPU those large models can be made, available to so many more people so I, hope that there will be a solution found, because if we were able to run those, large models on our CPU that is really, cool and to finally answer the question, yes both models can generate code but it, seems like the quantized version is not, as good as the original model version, which we would run on a GPU instead of a, CPU so let's hope that this can be fixed, in the future and now we can also run, the CPU version on our local computer, and also generate code with this and, this brings us to our next question do, these models even use or benefit from, gpus and the answer here is no that, inference is really computed only on the, CPU so there's no need for a GPU if, you're use the Dalai library but as I, showed you in the question before it has, also a way to run those models with a, GPU and as for now it seems that those, models work better than the quantized, version but I hope that will be solved, soon next question how much RAM is, recommended to run the individual models, on your computer and here I found this, piece of information where you can see, the recommended and RAM on your computer, for each model variant one user has, stated though that for the 30 billion, parameter variant he even needed 24, gigabytes of RAM and here's another, question does the model work with, languages other than English like, Spanish for example and the answer to, this can be found in the Llama paper and, here we can see that the model was, trained using various data sets and only, the Wikipedia data set consists of, multiple languages which are in total 20, so the the foundation model has seen, different languages but of course the, best results he will achieve in English, language so I think to really get great, results a fine tuning will be needed and, one more thing the alpaca model as you, can see and the prompt to generate the, training letter the instruction should, be in English only so the alpaca model, will be even more specialized on English, language so don't expect great results, in languages other than English using, the alpaca model alright another, question could we use this to query a, database using natural language, and the answer is not yet but the alpaca, model is already able to generate SQL, code but I could imagine that further, fine-tuning the model with SQL data, would definitely improve your results, and right now the model can just respond, to you but can't really actually query a, database but I'm planning to cover in, future videos how these broad large, language models can be combined with, domain expert models so if you're, curious about those topics feel free to, subscribe to my channel so then you, don't miss out on my future videos about, that and I got even more questions of, you guys and that's why I decided to, create a medium article where I answer, even more questions additional to this, video because I feel like some questions, can just be answered faster in written, form and here you can see all the, questions that are answered there are, also questions that I didn't cover in, this video for example why does the, model Generate random character, sequences so if your questions haven't, been answered in this video maybe check, out the document for me it's also easier, way to keep the document updated with, future questions because I can't really, update the video but updating the, document is quite easy for me and yeah, that's it for today's video One More, Time thank you for your incredible, feedback on my last video I hope today, you learned even more about dollop, hacker and the Llama model if so as, always I would appreciate if you give, this video a thumbs up also if you would, subscribe to my channel and until then, have a great time and see you in the, next video bye"
}